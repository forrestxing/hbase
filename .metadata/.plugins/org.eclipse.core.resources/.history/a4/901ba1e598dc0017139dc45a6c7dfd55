package com.hbase.cn.hbase.mapreduce;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FsShell;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.IOException;

public class MapperForBulkload {
	static Logger logger = LoggerFactory.getLogger(BulkLoadJob.class);

	public static void main(String[] args) {

	}

	public static class BulkLoadMap extends
			Mapper<LongWritable, Text, ImmutableBytesWritable, Put> {

		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {

			String[] valueStrSplit = value.toString().split("\t");
			String hkey = valueStrSplit[0];
			String family = valueStrSplit[1].split(":")[0];
			String column = valueStrSplit[1].split(":")[1];
			String hvalue = valueStrSplit[2];
			final byte[] rowKey = Bytes.toBytes(hkey);
			final ImmutableBytesWritable HKey = new ImmutableBytesWritable(
					rowKey);
			Put HPut = new Put(rowKey);
			byte[] cell = Bytes.toBytes(hvalue);
			HPut.add(Bytes.toBytes(family), Bytes.toBytes(column), cell);
			context.write(HKey, HPut);

		}
	}

}
